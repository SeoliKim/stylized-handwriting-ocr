{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa8960f",
   "metadata": {},
   "source": [
    "# Code Overview: Update these constants before running this code:\n",
    "---\n",
    "Before loading this data, run 'iam/dataset/process_dataset.ipynb' and 'imgur/dataset/process_dataset.ipynb'. The Data Processing section assumes the data has been loaded in the same way according to those files.\n",
    "When loading this data, update the directory constants 'IMGUR_DATA_DIR' and 'IAM_DATA_DIR' with your own directories:\n",
    "- The Imgur directory should contain 11 pickle files of word data images.\n",
    "- The IAM directory should contain a 'words.txt' file containing the text labels and '/words' subfolder containing additional organized subfolders of word images.\n",
    "\n",
    "Before training the model, update the model constant 'MODEL' to the model of your choice:\n",
    "- 'microsoft/trocr-base-handwritten'\n",
    "- 'microsoft/trocr-base-stage1'\n",
    "- 'microsoft/trocr-small-stage1'\n",
    "\n",
    "Update the model's output directory constant 'OUTPUT_DIR' to a directory of your choice. If you are running and saving multiple models, change this output directory before each run or the save will be overwritten.\n",
    "\n",
    "Before evaluating the model, update the saved model checkpoint directory 'CHECKPOINT_DIR' to the saved model checkpoint that you would like to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3434e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these constants with your own directories\n",
    "IMGUR_DATA_DIR = '/home/user/imgur' # Directory of Imgur data\n",
    "IAM_DATA_DIR = '/home/user/iam' # Directory of IAM data - should contain '/words' subfolder containing the word images and 'words.txt' file containing the text labels\n",
    "MODEL = 'microsoft/trocr-base-handwritten' # change to the model you would like to use\n",
    "OUTPUT_DIR = '/home/user/output/models' # Directory to save the model\n",
    "CHECKPOINT_DIR = '/home/user/output/models/checkpoint-####' # Directory of the saved model checkpoint that you would like to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac03f0c",
   "metadata": {},
   "source": [
    "## Step 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51cb85f",
   "metadata": {},
   "source": [
    "### 1.1 Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaeca27-793b-4000-9f6b-b6a51debd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to your desired directory\n",
    "os.chdir(IMGUR_DATA_DIR) # change $USER to netid\n",
    "\n",
    "# Confirm it's changed\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab61480",
   "metadata": {},
   "source": [
    "Imgur Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3745fd7-48a8-461e-9780-b5e110b5b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./dfwords_0_20000.pkl', 'rb') as file:\n",
    "    imgur_df1 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_20000_40000.pkl', 'rb') as file:\n",
    "    imgur_df2 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_40000_60000.pkl', 'rb') as file:\n",
    "    imgur_df3 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_60000_80000.pkl', 'rb') as file:\n",
    "    imgur_df4 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_80000_100000.pkl', 'rb') as file:\n",
    "    imgur_df5 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_100000_120000.pkl', 'rb') as file:\n",
    "    imgur_df6 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_120000_140000.pkl', 'rb') as file:\n",
    "    imgur_df7 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_140000_160000.pkl', 'rb') as file:\n",
    "    imgur_df8 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_160000_180000.pkl', 'rb') as file:\n",
    "    imgur_df9 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_180000_200000.pkl', 'rb') as file:\n",
    "    imgur_df10 = pickle.load(file)\n",
    "\n",
    "with open('./dfwords_200000_227055.pkl', 'rb') as file:\n",
    "    imgur_df11 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410eee9-1cdb-4dae-b9eb-b8af757900a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "imgur_df = pd.concat([imgur_df1, imgur_df2], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df3], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df4], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df5], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df6], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df7], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df8], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df9], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df10], ignore_index=True)\n",
    "imgur_df = pd.concat([imgur_df, imgur_df11], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4385c43",
   "metadata": {},
   "source": [
    "IAM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "label_file_path = IAM_DATA_DIR + '\\words.txt'\n",
    "image_file_path = IAM_DATA_DIR + '\\words'\n",
    "\n",
    "data = []\n",
    "with open(label_file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for idx, line in enumerate(lines[18:]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Processing line {idx}\")\n",
    "    row = []\n",
    "    tokens = line.strip().split()\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "\n",
    "    subfolder = tokens[0].split('-')[0]\n",
    "    subfolder2 = subfolder + \"-\" + tokens[0].split('-')[1]\n",
    "    image_file_name = subfolder + \"\\\\\" + subfolder2 + \"\\\\\" + tokens[0] + \".png\"\n",
    "    image_path = os.path.join(image_file_path, image_file_name)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.size[0] >= 10 and img.size[1] >= 10:\n",
    "                img_rgb = img.convert(\"RGB\")  # Convert to RGB\n",
    "                img_copy = img_rgb.copy()     # Copy after conversion\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Image file not found: {image_file_path}. Error: {e}\")\n",
    "        continue\n",
    "    except Image.UnidentifiedImageError as e:\n",
    "        print(f\"Unidentified image error for file {image_file_path}: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image file {image_file_path}: {e}\")\n",
    "        continue\n",
    "    row = [image_path, tokens[1], tokens[2], tokens[-1], img_copy]\n",
    "    # if len(row) != 10:\n",
    "    #     print(f\"Row length mismatch: {len(row)} elements in row: {row}\")\n",
    "    #     continue\n",
    "    data.append(row)\n",
    "\n",
    "\n",
    "print(f\"Length of a row in data: {len(data[0])}\")  # Should print 10\n",
    "\n",
    "print(data[0])\n",
    "iam_df = pd.DataFrame(data, columns=['id','text', 'image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd39ed-8ff9-47bb-815e-254681d06606",
   "metadata": {},
   "source": [
    "### 1.2 Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a738eea",
   "metadata": {},
   "source": [
    "Before cleaning the data, set loaded_dfwords to the data frame with the data you would like to clean (either 'imgur_df' or 'iam_df'). This code is the same for cleaning both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dfwords = df # replace 'df' with either 'imgur_df' or 'iam_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a925a60-56a3-401e-98a6-b45d5ebfb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image(row):\n",
    "    plt.imshow(loaded_dfwords.iloc[row,2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4040a5a-2c6b-4942-81cc-78890fa1eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "allowed_pattern = r'^[\\w\\s\\.,!?;:\\-+*/=()\\[\\]{}<>@#\\$%^&_\\'\"\\t\\n]+$'\n",
    "loaded_dfwords['text'] = loaded_dfwords['text'].str.replace('\\\\/', '/', regex=False)\n",
    "mask = ~loaded_dfwords['text'].str.contains(allowed_pattern, regex=True)\n",
    "loaded_dfwords=loaded_dfwords[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c0c3563-a750-4747-bcdf-76fa0bde883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dfwords = loaded_dfwords[loaded_dfwords['text'] != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dfwords = loaded_dfwords[loaded_dfwords['text'] != '-----------------------------------------------------']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dfwords.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289af3a-c757-493b-a5ba-ed274414bfc1",
   "metadata": {},
   "source": [
    "### 1.3 Splitting the Data into Training and Testing Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a35baa5f-4657-4390-b532-ee7ab81b6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get unique groups\n",
    "unique_images = loaded_dfwords['id'].unique()\n",
    "\n",
    "\n",
    "# Randomly select 10% for test \n",
    "np.random.seed(42)\n",
    "test_images = np.random.choice(unique_images, \n",
    "                              size=int(len(unique_images)*0.2), \n",
    "                              replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "991a2758-4458-4533-b933-d77fe786cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = loaded_dfwords[loaded_dfwords['id'].isin(test_images)]\n",
    "training_df = loaded_dfwords[~loaded_dfwords['id'].isin(test_images)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd7550-02d4-4e43-8e9c-71cf189b9852",
   "metadata": {},
   "source": [
    "### 1.4 Splitting the Training Data into Training and Validation Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64cfb5e0-bc23-4659-97ae-765f5de49da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "train_df, eval_df = train_test_split(training_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df = eval_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53584f-9d1b-495b-b632-b8bf0c6c7ae9",
   "metadata": {},
   "source": [
    "## Step 2. Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c9281",
   "metadata": {},
   "source": [
    "Reminder to update model constant 'MODEL' with the model of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9affb4-9a40-494f-b130-a552abc03e90",
   "metadata": {},
   "source": [
    "### 2.1 Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f67088bd-67e2-4f51-90a1-3a0d781cd11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c62a4e-0326-4fce-93cd-56df14603a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get base model\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc22f4",
   "metadata": {},
   "source": [
    "### 2.2 Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02e1dba6-94b3-442c-9a3e-92b86fd8cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class StyleDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_target_length=512):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      try:\n",
    "          text = self.df['text'][idx]\n",
    "          if not isinstance(text, str) or not text.strip():\n",
    "              raise ValueError(f\"Invalid text at index {idx}: {repr(text)}\")\n",
    "          image_id = self.df['id'][idx]\n",
    "          try:\n",
    "              image = self.df['image'][idx]\n",
    "          except Exception as e:\n",
    "              raise ValueError(f\"Failed to load image for ID {image_id} at index {idx}\") from e\n",
    "          try:\n",
    "              pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "          except Exception as e:\n",
    "              raise ValueError(f\"Image processing failed at index {idx}\") from e\n",
    "\n",
    "          if torch.isnan(pixel_values).any() or torch.isinf(pixel_values).any():\n",
    "              raise ValueError(f\"Invalid pixel values (NaN/inf) at index {idx}\")\n",
    "          try:\n",
    "              labels = self.processor.tokenizer(\n",
    "                  text,\n",
    "                  padding=\"max_length\",\n",
    "                  max_length=self.max_target_length\n",
    "              ).input_ids\n",
    "          except Exception as e:\n",
    "              raise ValueError(f\"Tokenization failed for text at index {idx}\") from e\n",
    "\n",
    "          # Replace pad_token_id with -100 for loss masking\n",
    "          labels = [\n",
    "              label if label != self.processor.tokenizer.pad_token_id else -100\n",
    "              for label in labels\n",
    "          ]\n",
    "          encoding = {\n",
    "              \"pixel_values\": pixel_values.squeeze(),\n",
    "              \"labels\": torch.tensor(labels)\n",
    "          }\n",
    "\n",
    "          if encoding[\"pixel_values\"].dim() != 3:\n",
    "              raise ValueError(f\"Invalid pixel_values shape at index {idx}\")\n",
    "\n",
    "          if encoding[\"labels\"].numel() != self.max_target_length:\n",
    "              raise ValueError(f\"Labels length mismatch at index {idx}\")\n",
    "\n",
    "          return encoding\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"\\nError in sample {idx}:\")\n",
    "          print(f\"   Error type: {type(e).__name__}\")\n",
    "          print(f\"   Details: {str(e)}\")\n",
    "          if hasattr(e, '__cause__') and e.__cause__:\n",
    "              print(f\"   Underlying error: {type(e.__cause__).__name__}: {str(e.__cause__)}\")\n",
    "          print(f\"   DataFrame row:\\n{self.df.iloc[idx]}\")\n",
    "          return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1a5efb3-dabc-4f81-b3f9-2d65d4fbe7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized\n",
    "train_dataset = StyleDataset(df=train_df,processor=processor)\n",
    "eval_dataset= StyleDataset(df=eval_df,processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93628f6a-a378-4c80-a5d2-a8745c655a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b3b92-0e22-4b3a-ae2c-cd13c153fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labeled word from dataset encoding\n",
    "def get_label_str(encoding):\n",
    "  labels = encoding['labels']\n",
    "  labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "  label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "  return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c488e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_label_str(train_dataset[0]))\n",
    "print(get_label_str(eval_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f386030-a888-493a-93ce-ef10097b8dd8",
   "metadata": {},
   "source": [
    "### 2.3 Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17936dd5-71b6-4e80-8e97-26f17fb6257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze your dataset first\n",
    "avg_target_len = training_df['text'].apply(len).mean()\n",
    "print(\"average target length\", avg_target_len)\n",
    "max_target_len = int(training_df['text'].apply(len).quantile(0.95))\n",
    "print(\"maximum target length\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8c70f-24ba-41c7-8933-6a3b0105cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=12,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.5,\n",
    "    no_repeat_ngram_size=3,\n",
    "    decoder_start_token_id=processor.tokenizer.cls_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token Alignment\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = len(processor.tokenizer)\n",
    "model.generation_config = generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34816fc-3bdc-4b85-ad98-981178c94e96",
   "metadata": {},
   "source": [
    "### 2.4 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1336000-01dc-412e-9b0e-4ed510e74ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412c306-b040-4fa3-ba01-e39531cec4bb",
   "metadata": {},
   "source": [
    "## Step 3. Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea0d48",
   "metadata": {},
   "source": [
    "Reminder to update model output directory 'OUTPUT_DIR' before fine-tuning. These configs are the same as the one we used to fine-tune each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742baafc-f4d5-4b3f-a265-961e33217a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=1.87,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,    # Essential for stability\n",
    "    lr_scheduler_type=\"cosine\",  # Smooth LR decay\n",
    "    fp16=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_steps=500,\n",
    "    save_steps=2000,\n",
    "    eval_steps=2000,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",       \n",
    "    greater_is_better=False,                \n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6d8ca-d8b1-4501-ba8a-0edda42aeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d0929",
   "metadata": {},
   "source": [
    "## Step 4. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ac428",
   "metadata": {},
   "source": [
    "### 4.1 Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers import TrOCRProcessor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(CHECKPOINT_DIR).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406afe3",
   "metadata": {},
   "source": [
    "### 4.2 Load Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should have been loaded from before in step 1.3\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f4c79",
   "metadata": {},
   "source": [
    "### 4.3 Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2d4a7",
   "metadata": {},
   "source": [
    "There is separate code for running inference on Imgur data and IAM data, since the image id and path to the image file is different. Make sure to run the code corresponding to the testing dataset used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318be43e",
   "metadata": {},
   "source": [
    "Model Inference on Imgur Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "def readText_batch(df, indices, model, processor):\n",
    "    \"\"\"Process multiple images at once\"\"\"\n",
    "    images= [df['image'][idx]for idx in indices]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    return processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "def process_all_rows_batched(df, model, processor, batch_size=8):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "        batch_indices = range(i, min(i+batch_size, len(df)))\n",
    "        try:\n",
    "            batch_texts = readText_batch(df, batch_indices,model, processor)\n",
    "            for idx, text in zip(batch_indices, batch_texts):\n",
    "                results.append({\n",
    "                    'id': df['id'][idx],\n",
    "                    'true_text': df['text'][idx],\n",
    "                    'predicted_text': text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size}: {str(e)}\")\n",
    "            for idx in batch_indices:\n",
    "                results.append({\n",
    "                    'id': df['id'][idx],\n",
    "                    'true_text': df['text'][idx],\n",
    "                    'predicted_text': None,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6566810",
   "metadata": {},
   "source": [
    "Model Inference on IAM data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "def get_image(df, image_id):\n",
    "    image_file_path = IAM_DATA_DIR + '/words'\n",
    "    subfolder = image_id.split('-')[0]\n",
    "    subfolder2 = subfolder + \"-\" + image_id.split('-')[1]\n",
    "    image_file_name = image_id + \".png\"\n",
    "    image_path = os.path.join(image_file_path, subfolder, subfolder2, image_file_name)\n",
    "    \n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.size[0] >= 10 and img.size[1] >= 10:\n",
    "                img_rgb = img.convert(\"RGB\")  # Convert to RGB\n",
    "                return img_rgb.copy()  # Return a copy after conversion\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image file {image_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2. Make Inference of the IAM-fine-tuned Base Model\n",
    "    # 2.1 Setup Target Model\n",
    "    # get targetted model\n",
    "    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "    processor = TrOCRProcessor.from_pretrained(processor)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model)\n",
    "    \n",
    "    \n",
    "# 2.2 Do Inference- OCRing\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "def readText_batch(df, indices, model, processor):\n",
    "    \"\"\"Process multiple images at once\"\"\"\n",
    "    images = [get_image(df, df['image_id'][index]) for index in indices]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    return processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "def process_all_rows_batched(df, model, processor, batch_size=8):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "        batch_indices = range(i, min(i+batch_size, len(df)))\n",
    "        try:\n",
    "            batch_texts = readText_batch(df, batch_indices,model, processor)\n",
    "            for idx, text in zip(batch_indices, batch_texts):\n",
    "                results.append({\n",
    "                    'id': df['id'][idx],\n",
    "                    'true_text': df['text'][idx],\n",
    "                    'predicted_text': text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size}: {str(e)}\")\n",
    "            for idx in batch_indices:\n",
    "                results.append({\n",
    "                    'id': df['id'][idx],\n",
    "                    'true_text': df['text'][idx],\n",
    "                    'predicted_text': None,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = process_all_rows_batched(test_df, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9abcc",
   "metadata": {},
   "source": [
    "### 4.4 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c282b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "cer = load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred_str, label_str):\n",
    "    pred_str=pred_str.strip()\n",
    "    label_str=label_str.strip()\n",
    "    try: \n",
    "        score = cer.compute(predictions=[pred_str], references=[label_str])\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(\"error\", e)\n",
    "        print(type(pred_str), len(pred_str), pred_str)\n",
    "        print(type(label_str), len(label_str), label_str)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e45c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()  \n",
    "# Run evalution\n",
    "results_df[\"metrics\"] = results_df.progress_apply(\n",
    "    lambda row: compute_metrics(row[\"predicted_text\"], row[\"true_text\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb3e31",
   "metadata": {},
   "source": [
    "### 4.4 Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87032d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_eval(values):\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.kdeplot(values, shade=True)\n",
    "    plt.xlabel(\"Edit Distance\")\n",
    "    plt.title(\"KDE of Edit Distances\")\n",
    "    plt.show()\n",
    "        \n",
    "    # Boxplot\n",
    "    plt.boxplot(values, vert=False, patch_artist=True)\n",
    "    plt.xlabel(\"Edit Distance\")\n",
    "    plt.title(\"Boxplot of Edit Distances\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval(results_df[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "results_normal= results_df[results_df[\"metrics\"]<5]\n",
    "plot_eval(results_normal[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda45cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_state(values):\n",
    "    stats = {\n",
    "        \"mean\": np.mean(values),\n",
    "        \"median\": np.median(values),\n",
    "        \"std\": np.std(values),\n",
    "        \"min\": np.min(values),\n",
    "        \"max\": np.max(values),\n",
    "        \"quantiles\": np.quantile(values, [0.25, 0.5, 0.75]),\n",
    "        \"perfect\": np.sum(values == 0)\n",
    "\n",
    "    }\n",
    "    \n",
    "    print(\"Summary Statistics:\")\n",
    "    print(f\"- Mean ± Std: {stats['mean']:.2f} ± {stats['std']:.2f}\")\n",
    "    print(f\"- Median (IQR): {stats['median']:.2f} ({stats['quantiles'][0]:.2f}–{stats['quantiles'][2]:.2f})\")\n",
    "    print(f\"- Range: [{stats['min']}, {stats['max']}]\")\n",
    "    print(f\"- Quantiles (25th, 50th, 75th): {stats['quantiles'].round(2)}\")\n",
    "    print(f\"- Perfect Predictions: {stats['perfect']} ({stats['perfect']/len(values)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_state(results_df[\"metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
